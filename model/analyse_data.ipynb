{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with normal beam 2, without minlength control\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "beam_nomulti = './scores/lang-att-beam2-30k_minlength.json'\n",
    "beam_multi = './scores/lang+att-beam2-30k_minlength.json'\n",
    "beam_nomulti_att = './scores/lang-att+beam2-30k_minlength.json'\n",
    "beam_multi_att = './scores/lang+att+beam2-30k_minlength.json'\n",
    "\n",
    "with open(beam_nomulti, 'r') as f:\n",
    "    beam_nomulti = json.load(f)\n",
    "with open(beam_multi, 'r') as k:\n",
    "    beam_multi = json.load(k)\n",
    "with open(beam_nomulti_att, 'r') as l:\n",
    "    beam_nomulti_att = json.load(l)\n",
    "with open(beam_multi_att, 'r') as p:\n",
    "    beam_multi_att = json.load(p)\n",
    "    \n",
    "hypotheses = [beam_nomulti, beam_multi, beam_nomulti_att, beam_multi_att]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hyp(data_dict):\n",
    "    hyps_str = ''\n",
    "    for item in data_dict:\n",
    "        hyps_str += item['hypotheses']\n",
    "        hyps_str += ' '\n",
    "    out = nltk.tokenize.word_tokenize(hyps_str)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152986\n",
      "400\n",
      "\n",
      "151964\n",
      "425\n",
      "\n",
      "153051\n",
      "400\n",
      "\n",
      "152552\n",
      "434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size (number of types)\n",
    "\n",
    "for each_set in hypotheses:\n",
    "    this_set_hyps_words = extract_hyp(each_set)\n",
    "    print(len(this_set_hyps_words))\n",
    "    print(len(set(this_set_hyps_words)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37949\n",
      "295\n",
      "\n",
      "38354\n",
      "320\n",
      "\n",
      "38489\n",
      "296\n",
      "\n",
      "38489\n",
      "325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of nouns and unique nouns (nouns describe objects, so it is a rough indicator of object mention)\n",
    "\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "for each_set in hypotheses:\n",
    "    this_set_hyps_words = extract_hyp(each_set)\n",
    "    this_set_nouns = [word for (word, pos) in nltk.pos_tag(this_set_hyps_words) if is_noun(pos)]\n",
    "    print(len(this_set_nouns))\n",
    "    print(len(set(this_set_nouns)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetetive_nouns(data_dict):\n",
    "    all_ratios = 0\n",
    "    for item in data_dict:\n",
    "        hyp = item['hypotheses'].split('.')\n",
    "        this_hyp_nouns = {}\n",
    "        num_of_rep_nouns = 0\n",
    "        for elem in hyp:\n",
    "            out = nltk.tokenize.word_tokenize(elem)\n",
    "            nouns = [word for (word, pos) in nltk.pos_tag(out) if is_noun(pos)]\n",
    "            for n in nouns:\n",
    "                if n not in this_hyp_nouns:\n",
    "                    this_hyp_nouns[n] = 1\n",
    "                else:\n",
    "                    this_hyp_nouns[n] += 1\n",
    "        for each_n, freq in this_hyp_nouns.items():\n",
    "            if freq != 1:\n",
    "                # if some noun repeats > 1 times\n",
    "                num_of_rep_nouns += 1\n",
    "        ratio = num_of_rep_nouns / len(this_hyp_nouns)\n",
    "        all_ratios += ratio\n",
    "    return all_ratios / len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49193992360926664\n",
      "0.4729834119498632\n",
      "0.4914266450927754\n",
      "0.4774229620653875\n"
     ]
    }
   ],
   "source": [
    "# average ratio of repetetive nouns / all unique nouns per paragraph\n",
    "# smaller means more unique nouns in a generated paragraph on average\n",
    "# nouns repeated more than once / all nouns\n",
    "\n",
    "# 'in a single paragraph on average, 49% of the nouns are occuring more than 1 time in its sentences'\n",
    "# models with multimodal input demonstrate more diversity in terms of nouns they contain\n",
    "# this number should be high enough for connectivity, but not too much\n",
    "# IDEA: introduce n-gram penalty on the level of words and/or the level of sentences?\n",
    "# this would make av_rep_ratio_noun 'better', and improve generation evaluation scores\n",
    "\n",
    "for each_set in hypotheses:\n",
    "    av_rep_ratio_noun = repetetive_nouns(each_set)\n",
    "    print(av_rep_ratio_noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   |                                 | # of words | # of types | # of nouns | # of unique nouns |\n",
    "|---|---------------------------------|------------|------------|------------|-------------------|\n",
    "|   | BASELINE MULTIMODAL- ATTENTION- | 115,753    | 413        | 28,337     | 308               |\n",
    "|   | BASELINE MULTIMODAL+ ATTENTION- | 116,381    | 437        | 28,755     | 321               |\n",
    "|   | BASELINE MULTIMODAL- ATTENTION+ | 118,298    | 416        | 29,094     | 317               |\n",
    "|   | BASELINE MULTIMODAL+ ATTENTION+ | 113,042    | 441        | 27,914     | 327               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "\n",
    "VISUAL = './scores/VISUAL-beam_2_cidertest.json'\n",
    "LANGUAGE = './scores/LANGUAGE-beam_2_cidertest.json'\n",
    "VISUAL_ATT = './scores/VISUAL_ATTENTION-beam_2_cidertest.json'\n",
    "LANGUAGE_ATT = './scores/LANGUAGE_ATTENTION-beam_2_cidertest.json'\n",
    "VISUAL_LANGUAGE = './scores/VISUAL_LANGUAGE-beam_2_cidertest.json'\n",
    "VISUAL_LANGUAGE_ATT = './scores/VISUAL_LANG_ATTENTION_NOFUSION-beam_2_cidertest.json'\n",
    "\n",
    "with open(VISUAL, 'r') as f:\n",
    "    VISUAL = json.load(f)\n",
    "with open(LANGUAGE, 'r') as l:\n",
    "    LANGUAGE = json.load(l)\n",
    "with open(VISUAL_ATT, 'r') as f:\n",
    "    VISUAL_ATT = json.load(f)\n",
    "with open(LANGUAGE_ATT, 'r') as l:\n",
    "    LANGUAGE_ATT = json.load(l)\n",
    "with open(VISUAL_LANGUAGE, 'r') as f:\n",
    "    VISUAL_LANGUAGE = json.load(f)\n",
    "with open(VISUAL_LANGUAGE_ATT, 'r') as l:\n",
    "    VISUAL_LANGUAGE_ATT = json.load(l)\n",
    "    \n",
    "with open('/home/xilini/par-gen/01-par-gen/data/paragraphs_v1.json', 'r') as j:\n",
    "    ground_truth = json.load(j)\n",
    "with open('/home/xilini/par-gen/01-par-gen/data/splits.json', 'r') as s:\n",
    "    splits = json.load(s)\n",
    "\n",
    "gt = []\n",
    "for elem in ground_truth:\n",
    "    if splits[str(elem['image_id'])] == 'test':\n",
    "        this_gt = {}\n",
    "        this_gt['image_id'] = str(elem['image_id'])\n",
    "        this_gt['hypotheses'] = elem['paragraph']\n",
    "        gt.append(this_gt)\n",
    "    \n",
    "\n",
    "hypotheses = [\n",
    "    VISUAL,\n",
    "    VISUAL_ATT,\n",
    "    LANGUAGE,\n",
    "    LANGUAGE_ATT,\n",
    "    VISUAL_LANGUAGE,\n",
    "    VISUAL_LANGUAGE_ATT,\n",
    "    gt\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction 1\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hyp(data_dict):\n",
    "    hyps_str = ''\n",
    "    for item in data_dict:\n",
    "        hyps_str += item['hypotheses']\n",
    "        hyps_str += ' '\n",
    "    #out = [w.text for w in tokenizer(hyps_str)]\n",
    "    #out = nltk.tokenize.word_tokenize(hyps_str)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93430\n",
      "403\n",
      "\n",
      "95993\n",
      "390\n",
      "\n",
      "91659\n",
      "389\n",
      "\n",
      "95047\n",
      "376\n",
      "\n",
      "95411\n",
      "358\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-eb1b0ee436e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypotheses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mthis_set_hyps_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_hyp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_set_hyps_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-0632a70dd9a6>\u001b[0m in \u001b[0;36mextract_hyp\u001b[0;34m(data_dict)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mhyps_str\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#out = [w.text for w in tokenizer(hyps_str)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vis-par/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/envs/vis-par/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/envs/vis-par/lib/python3.7/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENDING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# vocabulary size (number of types)\n",
    "\n",
    "for each_set in hypotheses:\n",
    "    this_set_hyps_words = extract_hyp(each_set)\n",
    "    \n",
    "    print(len(this_set_hyps_words))\n",
    "    print(len(set(this_set_hyps_words)))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nouns and unique nouns (nouns describe objects, so it is a rough indicator of object mention)\n",
    "\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "for each_set in hypotheses:\n",
    "    this_set_hyps_words = extract_hyp(each_set)\n",
    "    this_set_nouns = [word for (word, pos) in nltk.pos_tag(this_set_hyps_words) if is_noun(pos)]\n",
    "    print(len(this_set_nouns))\n",
    "    print(len(set(this_set_nouns)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tokenizer('The big thing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/xilini/par-gen/01-par-gen/')\n",
    "from evalfunc.bleu.bleu import Bleu\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_bleu(each_set):\n",
    "    \n",
    "    refs_all = {}\n",
    "    hyps_all = {}\n",
    "    \n",
    "    for item in each_set:\n",
    "        \n",
    "        this_image_id = str(item['image_id'])\n",
    "        this_hyps = item['hypotheses']\n",
    "        sents = nltk.sent_tokenize(this_hyps)\n",
    "        \n",
    "        for ref_n, each_s in enumerate(sents):\n",
    "            these_hyps = [each_s]\n",
    "            these_ref = []\n",
    "            for hyp_n, other_s in enumerate(sents):\n",
    "                if hyp_n != ref_n:\n",
    "                    these_ref.append(other_s)\n",
    "            if these_ref != []:\n",
    "                refs_all[this_image_id + '_' + str(ref_n)] = these_ref\n",
    "                hyps_all[this_image_id + '_' + str(ref_n)] = these_hyps\n",
    "            \n",
    "    return refs_all, hyps_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorers = [\n",
    "    (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "]\n",
    "\n",
    "for each_set in hypotheses:\n",
    "    \n",
    "    refs, hyps = self_bleu(each_set)\n",
    "\n",
    "    score = []\n",
    "    method = []\n",
    "    for scorer, method_i in scorers:\n",
    "\n",
    "        score_i, _ = scorer.compute_score(refs, hyps)\n",
    "        score.extend(score_i) if isinstance(score_i, list) else score.append(score_i)\n",
    "        method.extend(method_i) if isinstance(method_i, list) else method.append(method_i)\n",
    "\n",
    "    score_dict = dict(zip(method, score))\n",
    "    \n",
    "    print(score_dict)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetetive_nouns(data_dict):\n",
    "    all_ratios = 0\n",
    "    for item in data_dict:\n",
    "        hyp = item['hypotheses'].split('.')\n",
    "        this_hyp_nouns = {}\n",
    "        num_of_rep_nouns = 0\n",
    "        for elem in hyp:\n",
    "            out = nltk.tokenize.word_tokenize(elem)\n",
    "            nouns = [word for (word, pos) in nltk.pos_tag(out) if is_noun(pos)]\n",
    "            for n in nouns:\n",
    "                if n not in this_hyp_nouns:\n",
    "                    this_hyp_nouns[n] = 1\n",
    "                else:\n",
    "                    this_hyp_nouns[n] += 1\n",
    "        for each_n, freq in this_hyp_nouns.items():\n",
    "            if freq != 1:\n",
    "                # if some noun repeats > 1 times\n",
    "                num_of_rep_nouns += 1\n",
    "        ratio = num_of_rep_nouns / len(this_hyp_nouns)\n",
    "        all_ratios += ratio\n",
    "    return all_ratios / len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average ratio of repetetive nouns / all unique nouns per paragraph\n",
    "# smaller means more unique nouns in a generated paragraph on average\n",
    "# nouns repeated more than once / all nouns\n",
    "\n",
    "# 'in a single paragraph on average, 49% of the nouns are occuring more than 1 time in its sentences'\n",
    "# models with multimodal input demonstrate more diversity in terms of nouns they contain\n",
    "# this number should be high enough for connectivity, but not too much\n",
    "# IDEA: introduce n-gram penalty on the level of words and/or the level of sentences?\n",
    "# this would make av_rep_ratio_noun 'better', and improve generation evaluation scores\n",
    "\n",
    "for each_set in hypotheses:\n",
    "    av_rep_ratio_noun = repetetive_nouns(each_set)\n",
    "    print(av_rep_ratio_noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   |                                 | # of words | # of types | # of nouns | # of unique nouns | % of REP NOUNS |\n",
    "|---|---------------------------------|------------|------------|------------|-------------------|-----------|\n",
    "|   | BASELINE MULTIMODAL- ATTENTION- | 115,753    | 413        | 28,337     | 308               | 43        |\n",
    "|   | BASELINE MULTIMODAL+ ATTENTION- | 116,381    | 437        | 28,755     | 321               | 42        |\n",
    "|   | BASELINE MULTIMODAL- ATTENTION+ | 118,298    | 416        | 29,094     | 317               | 41        |\n",
    "|   | BASELINE MULTIMODAL+ ATTENTION+ | 113,042    | 441        | 27,914     | 327               | 39        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_overlap(data_dict):\n",
    "    \n",
    "    item_count = 0\n",
    "    item_count_not = 0\n",
    "    \n",
    "    # get all unique words from ground-truth paragraphs\n",
    "    gt_str = ''\n",
    "    for item in gt:\n",
    "        gt_str += item['hypotheses']\n",
    "        gt_str += ' '\n",
    "    out = nltk.tokenize.word_tokenize(gt_str)\n",
    "    out = set(out)\n",
    "    \n",
    "    # get all unique words in our generations\n",
    "    item_str = ''\n",
    "    for item in data_dict:\n",
    "        item_str += item['hypotheses']\n",
    "        item_str += ' '\n",
    "    item_out = nltk.tokenize.word_tokenize(item_str)\n",
    "    item_out = set(item_out)\n",
    "    \n",
    "    for i in item_out:\n",
    "        if i in out:\n",
    "            item_count += 1\n",
    "        else:\n",
    "            item_count_not += 1\n",
    "            \n",
    "    item_count = (item_count * 100) / len(out)\n",
    "    \n",
    "    return item_count, 100 - item_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlap in terms of generated words\n",
    "# which model overlaps more with words in ground truth\n",
    "\n",
    "hypotheses = [beam_nomulti, beam_multi, beam_nomulti_att, beam_multi_att]\n",
    "for each_set in hypotheses:\n",
    "    overlapped, not_overlapped = word_overlap(each_set)\n",
    "    print(overlapped, not_overlapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

Nikolai

## 2020-05-15 

  - Extracted DenseCap features
  - Transformer presentation
  - Multi-modal fusion possibilities:
	* pool(visual), pool (backrgound), visual + background = 512 + 512 = 1024; confuses end of sentence detection but produces cool and rich descriptions ==> move end of sentence detection (possibly to the end of the word LSTM) ; remove end of sentence detection completely
	* combinate visual and background into a single 512 vector with multiplication and test; does this improve end of sentence detection and also generates interesting descriptions
	* no pooling, hence 50 x 1024 or 50 x 512; then a dense layer to 512
  - Conceptual model overview:
	* Information fusion (visual + background) and summarisation (pooling)
	* Discourse planning (sentence LSTM)
	* Sentence relaisation (word LSTM)
	* Where does EOP (end of paragraph) fit in?
	* See our diagram doodle
  - Replace DenseCap (2016) with FasterRCNN (bottom-up, Anderson; Detectron2 SoTA for object detection by FB) https://github.com/facebookresearch/detectron2 
  - Numeric prediction for EOP which conveys gradience for individual sentences leading to the stop (maybe this is not relevant since the EOP is not an LSTM (the signal about EOP is not fed back into prediction) but this relies on the sentence LSTM which is gradient)
  


## 2020-05-08 

  - Sentence LSTM, add gradient scores for the sentence completion based on the degree to which the sentence is the last sentence
  - Presentation in the Transforms course: S. Herdade, A. Kappeler, K. Boakye, and J. Soares. Image captioning: Transforming objects into words. In Advances in Neural Information Processing Systems, pages 11135–11145, 2019.
  - Paper 1: Intent in image captioning
  - Paper 2: Adding language/conceptual backrgound information to paragrpah description: Coling, deadline 2020-07-01, alternative LANTERN workshop, deadline 2020-08-21
  - Paper 3: Transformer paper, compare the vision and language fusion between trasnformers and adaptive attention
  - Realtion to attention and information fusion
    * S. Dobnik and J. D. Kelleher. A model for attention-driven judgements in Type Theory with Records. In J. Hunter, M. Simons, and M. Stone, editors, JerSem: The 20th Workshop on the Semantics and Pragmatics of Dialogue, volume 20, pages 25–34, New Brunswick, NJ USA, July 16–18 2016.



## 2020-05-01 

  - How to add attention (similar to Lu and Ghanimifard):
  * In addition to visual embedding we also have ambedding from the phrases generated by DenseCap
  * attend on a vector that contains visual embeddings and phrases from the DenseCap
  * Motivation: DenseCap descriptions will be descriptions of the most visible objects in the scene; in DenseCap we can control the number of region proposals from which unique objects are identified (there are constraints on how objects are combined); the number of identified objects is not the same for images; (1) we could take the visual features of these identified objects as a vector of attended objects for example; (2) we take the actual LSTM hidden state of the generated descriptions of these proposals; in both cases we are testing the effects of transfer learning
  * MaxPooling is now performing the task of attention; MaxPooling also from DenseCap regions and DenseCap description vectors; maybe in later versions replace with attention
  * Where to introduce attention; before Sentence LSTM to generate sentence topics or at the level of Word LSTM to generate individual words?
  * 
  * S. Ullman. Visual routines. Cognition, 18(1–3):97–159, 1984.
  - Where to introduce attention?
	* Attention over visual features and the DenseCap features: what is relevant in the image to describe?
	* Attention on sentence LSTM: how to describe rleevant information accross sentences
	* Attention over words: how to generate individual words in a sentence
  - Next:
	* Extract linguistic information from DenseCap and add it to the model with MaxPooling; (1) visual representations of the identified regions (2) LSTM language model of the identified regions; (3) both
	   * Getting the automatic scores for the current models
	* Calculations of CIDEr: how to interpret the score; experiment with ground truth being slightly randomly modified; the effect on the score
  - Pragmatic descriptions of perceptual stimuli Emiel van Miltenberg: [paper](https://www.aclweb.org/anthology/E17-4001.pdf)
   * Nucleus model with temperature is the best (comparing the generations manually) 

On the intent in image description tasks

  - The task of image captioning/descriptions is unnatural; there is no clearly expressed intent which is defined by the task
  - The images are taken with a particular intent but this is not visible to captioners (e.g. the beach and the climbing boy example)
  - This leads to problems:
	* The quality of the descriptions: people are unsure what they should be describing; they resort to very generic descriptions: highly technical, e.g. man woman, etc. quite uninteresting; the problem of evaluation?
	* The quality of the images: they are not representing very typical scenes sometimes and therefore relations are less visual; even describers halucinate/rely on their world knowledge
	* Sometimes the descriptions that are generated are valid but different from the ground truth; we do not know whether the system is looking at what it is describing or halucinating; there may be several different tasks/intention for a particula picture; the system may have identified that from the LM but it is not grounded
	- But even if we know the intent; then the system needs to learn a discousre model of a particular task/intent; what are valid sequences of sentences in a paragrpH, how dow e approach to describe a particular task; generate a general description and then focus on individual details; paragraph discourse model
	_ The role of attention: the model needs to learn what features to attend on;: the attention is at different levels; the image, general topics; how to structure the paragraphs; how to generate individual words in a sentence
	- Keywords:
	  * visual data
	  * task orintedness
	  * attention


## 2020-04-23 

  - Baseline model with no pre-training now generates very good descriptions; the problem was in the way the generation was implementaed; it was conditioned on the ground truth rather than previous prediction
  - Different implementations of search
	* Greedy search: generic output, frequenetly incorrect and repetitive
	* Sampling from multinomial distribution https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.multinomial.html for implementation see Softmax-with-temperature-test.ods
	* Top-n sampling (temperature scaling on the top most probable words), https://cs.stackexchange.com/questions/79241/what-is-temperature-in-lstm-and-neural-networks-generally
	* Beam-search
	

## 2020-04-17 

  - Using Densecap embeddings and LSTM language model and fine-tuning them (initilaise only and then update the weights) improves the performance
  - A system without the denscap language model generates fragmented phrases (which seem to be good) but the syntax is fragmented
  - In packing sentences, now we do not calculate loss on padded tokens
  - Results: 
	* Baseline 1: 
  - Next?
	* Double checking of the code
	* Create Topic/language embeddings: take an image, create denscap descriptions, take the hidden state of the last word of the LSTM and save this as a topic represnetation for each region; 50 regions; then max polling and finally we get Topic embeddings; concatenate topic embeddings and visual embeddings; start generating sentences from that; visual and language pipeline; the sentence genrator needs to learn how to select information and generate from that
	* INLG, 15 August
	* Any other intermediate conferences
	* Semdial, 15 May: a position paper about the state of the art of the paragraph generation
	* Literature: Devi Parikh (Drev Batra), deep learning models for generation; how are the intermediate topics represented from which language is generated
	* A comparison of paragraph generation models: https://helda.helsinki.fi/bitstream/handle/10138/304686/arturs_polis_thesis_final.pdf?sequence=1&isAllowed=y
	* Kevin Knight, The Moment When the Future Fell Asleep 9am-10am, June 3, NAACL 2018
	* Elizabeth Clark, Yangfeng Ji, Noah A. Smith: Neural Text Generation in Stories Using Entity Representations as Context, NAACL 2018 outstanding paper, 4:36PM - 4:54PM, https://www.aclweb.org/anthology/N18-1204/ (includes [video](https://vimeo.com/277672801) of the talk)



## 2020-04-10

  - The graph of the learning configuration, 2020-04-10-model_scheme.pdf
  - Test the effects of the end of sentence predictor by removing this part
  - Remove one of the RuLUs re-econding the visual features
  - End to end model from visual features to sentences; the model does not have any attention; can we help the model to attend; dense-cap descriptions; attention on visual features in the bounding boxes (cf. Mehdi's paper)
  - Training and validation loss: after some point the validation loss explodes and training loss is reduced: over-fitting starts; descriptions at this stage?
  - Pre-training in the original paper:
    * Word-LSTM (embeddings, weights and linear) is pre-trained from Densecap; this is fine-tuned for the current sentences
    * Word-embeddings from the Densecap
    * What happens if we use a pre-trained language model instead of training it?
  - Next?
    * Baseline 1: Plot the training and validation loss, validate the generation of the model with the lowest val loss; save the sentences
    * Baseline 2: Remove the multi-tasking of the sentence loss; improvement? Generate max number of images, i.e. 6
    * Baseline 3: Representation of visual vectors, remove one of the ReLUs
    * Baseline 4: Use Densecap pre-training for the word LSTMs; two options: freeze and add another layer; use the weights and update them in the fine tuning; there may be doing this in the paper
  - INLG: https://www.inlg2020.org,  deadline of 15 May



## 2020-04-03

  - Discussing the ISP



## 2020-03-27

  - Passing the LSTM cell state of the last word of the previous sentence as an input to the sentence LSTM makes a difference in predicting whether this is the last sentence
  - The semantics of the previous sentence (word-embeddings)
  - the effects of the batch size on training
  - The curiousc case of loss:
    * validation loss is initially low, after a few epochs is equal and then subseqwuently slighty higher but still following training loss and decreasing; we cut off training when the validation loss increases
    * large batch sizes require more training, but the overall loss is lower and overfitting happens later; large batch sizes are conservative in updates but they nonetheless achieve overall lower loss
    * experiment: start training with 128 BS and then later increase the batch size; and then increase the BS to 256; large BS as fine tuning? S's expectation is that this will only speed up the training; N's expectation is that we would actually reach lower validation; compare where BS is constant, e.g. 256 with a case where BS is first 128 and then switched to 256; a case of fine tuning
    * experiment: represent dense-cap captions as a document topic vector; NAACL in New Orleans a keynote on poetry egenration; a best paper on story generation; both papers some represnettaion; having denscap texts summarised as a vector would tell us what is interesting in the picture; attention on what is interesting to describe in this picture for humans;
    * pass the densacap through some sentence embeddings encoder; pre-trained embeddings for dense-caps, encode densecaps to a semantics vector which represents the attention through language in the model
    
  


## 2020-03-20

2020-03-20-generation-model.png

  - loss: removed the loss on the padding
  - removed the vector that we considered to be the topic vector
  - diagram showing the model
    * connect the hidden state of the word LSTM as an input to the sentence LSTM as a topic encoder
  - What is the relevant thing we want to say about the image?
    * Currently, the module is based only image features
    * What do want to say about the image?
    * Use the last state of the word LSTM to encode the semantics of this sentence and concatenate this with the sentence LSTM as a topic of the previous sentence (currently the state of the last word LSTM is passed in as the hidden state of the first word of the next sentence
    * What dependencies do we want to encode: each sentence should represent an interesting sepearate topic; but there should also be dependencies between the sentneces, e.g. co-reference
    * Try the current model vs the sentence topic model
    * Then next xet of experiments, also include a topic model from the descriptions from the densecap
  - Document experiments and their results on Github as Wiki
  - ISP study plan
  - Next
    * Daily catchup meetings at 10:00 - 10:15
    * Meet on Tuesday


## 2020-02-26

  - Summary of the model
  - https://github.com/nilinykh/im2p_new
    * Resnet
    * Initialise the RNN with the image or 0s; if 0s then use image as an input; the zero initialisation makes more sense; image as an inout to every word (Karpathy says this reduces påerformance); currently 0 initialisation; Glorot initialisation
    * comet.ml
    * How to compare BLEU sentences; a generates sentence against every sentence
  - Datasets
    * Stanford
    * Your dataset
  - Densecap vs Resnet
  - Different evaluation scores
  - Generated sentences
  - https://cs.stanford.edu/people/ranjaykrishna/im2p/index.html
  - 
  


## 2020-02-18

  - Coling workshops: *SEM, Lantern
  - INLG paper on object linking
  - A Hierarchical Approach for Generating Descriptive Image Paragraphs: https://cs.stanford.edu/people/ranjaykrishna/im2p/index.html
    * DenseCap (Karpathy, small captions of regions), alternatives fasterRNNs, Yolo
    * Recurrent Topic-Transition GAN for Visual Paragraph Generation, Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan, Eric P. Xing: https://arxiv.org/abs/1703.07022
  - Todo
    * what objects are most relevant; prioritise detected objects and include them in the
  - M. Tanti and A. C. K. Gatt. Quantifying the amount of visual information used by neural caption generators. In Proceedings of the 1st Workshop on Shortcomings in Vision and Language (SiVL’18), ECCV, 2018.
  - M. Tanti, A. Gatt, and K. P. Camilleri. Where to put the image in an image caption generator. Natural Language Engineering, 24(3):467–489, 2018.
  - A. Ramisa, J. Wang, Y. Lu, E. Dellandrea, F. Moreno-Noguer, and R. Gaizauskas. Combining geometric, textual and visual features for predicting prepositions in image descriptions. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 214–220, Lisbon, Portugal, 7–21 September 2015. Association for Computational Linguistics.

    

\UseRawInputEncoding
\documentclass[notes=hide]{beamer}
\usepackage[german,english]{babel}
\usepackage{german}
\usetheme{Madrid}
\usepackage{floatflt}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{verbatim}
\usepackage{unicode-math}
\usepackage{pst-node}
\usepackage{fancyvrb}
\usepackage{moreverb}
\usecolortheme{lily}

%\definecolor{UBCblue}{rgb}{0.04706, 0.13725, 0.26667} % UBC Blue (primary)
%\usecolortheme[named=UBCblue]{structure}

\makeatother
\setbeamertemplate{footline}
{
  \leavevmode%
  \hbox{%
 % \begin{beamercolorbox}[wd=.45\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
   % \usebeamerfont{author in head/foot}\insertshortauthor
  %\end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.7\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \insertframenumber{} / \inserttotalframenumber\hspace*{1ex}
  \end{beamercolorbox}}%
  \vskip0pt%
}
\makeatletter
\setbeamertemplate{navigation symbols}{}

\definecolor{firstsent}{RGB}{51, 255, 153}
\definecolor{secondsent}{RGB}{255, 181, 112}
\definecolor{thirdsent}{RGB}{255, 0, 136}
\definecolor{fourthsent}{RGB}{255, 0, 255}
\newcommand\setR{\symbb{R}}
\newcommand{\eqname}[1]{\tag*{#1}}% Tag equation with name

%\renewcommand{\footnotesize}{\fontsize{7pt}{7pt}\selectfont}
%\addtolength{\footnotesep}{5mm} 
%\setlength\footnotemargin{1pt}


\title[INLG 2020, Oral Session 6: NLG and Vision]{When an Image Tells a Story: The Role of Visual and Semantic Information for Generating Paragraph Descriptions}
%\subtitle{INLG 2020 presentation}
\author{Nikolai IlinykhÂŸ, Simon Dobnik}
\institute{Centre for Linguistic Theory and Studies in Probability (CLASP) \\
               Department of Philosophy, Linguistics and Theory of Science (FLoV)\\
               University of Gothenburg, Sweden}

\date{17$^{th}$ December 2020}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

%\section{Describing images with longer sequences}

\begin{frame}{Describing images with longer sequences}
\small
\center
\fbox{\includegraphics[width=0.5\textwidth]{2327613}}
\end{frame}

\begin{frame}{Describing images with longer sequences\footnote{Krause, J., Johnson, J., Krishna, R., \& Fei-Fei, L. (2017). A Hierarchical Approach for Generating Descriptive Image Paragraphs. In Computer Vision and Pattern Recognition (CVPR).}}
\small
\center
\fbox{\includegraphics[width=0.5\textwidth]{2327613}}
\begin{block}{}
People are standing on the grass behind a concrete patch that looks like it was just set. There are two orange cones in front of the concrete and yellow tape surrounding it. There are three people in yellow vests and white hard hats. There are some people sitting on a bench next to them.
\end{block}
\end{frame}

\iffalse
\begin{frame}{Properties of Image Paragraphs (IP)}
\small
\includegraphics[width=.5\textwidth]{2327613boxes}
\begin{block}{}
\textcolor{firstsent}{\textbf{People}} are standing on the \textcolor{firstsent}{\textbf{grass}} behind \Rnode{st2}{\underline{\textcolor{firstsent}{\textbf{a concrete patch}}}} that looks like \Rnode{ss2}{\underline{it}} was just set. There are \textcolor{secondsent}{\textbf{two orange cones}} in front of  \Rnode{st1}{\underline{\textcolor{secondsent}{\textbf{the concrete and yellow tape}}}} surrounding \Rnode{ss1}{\underline{it}}. There are \Rnode{st}{ \underline{\textcolor{thirdsent}{\textbf{three people in yellow vests and white hard hats}}}}. There are \textcolor{fourthsent}{\textbf{some people sitting on a bench}} next to \Rnode{ss}{\underline{them}}.
\ncbar[arrows = ->, linecolor =blue, angle = -90, arm = 0.9em]{ss}{st}
\ncbar[arrows = ->, linecolor =blue, angle = 90, arm = 1.4em]{ss1}{st1}
\ncbar[arrows = ->, linecolor =blue, angle = 90, arm = 1.4em]{ss2}{st2}
\end{block}
\end{frame}
\fi

\begin{frame}{Two Sources of Important Information for IP}
\small
\begin{minipage}{0.5\textwidth}
\includegraphics[width=1\textwidth]{2327613boxes}
\end{minipage} \hfill
\begin{minipage}{0.45\textwidth}
\begin{enumerate}
\item visual features of perceived objects (\textit{what} to refer to)
\item background knowledge and communicative intent (\textit{when} and \textit{how} to refer)
\end{enumerate}
\end{minipage}
\begin{block}{}
\textcolor{firstsent}{\textbf{People}} are standing on the \textcolor{firstsent}{\textbf{grass}} behind \Rnode{st2}{\underline{\textcolor{firstsent}{\textbf{a concrete patch}}}} that looks like \Rnode{ss2}{\underline{it}} was just set. There are \textcolor{secondsent}{\textbf{two orange cones}} in front of  \Rnode{st1}{\underline{\textcolor{secondsent}{\textbf{the concrete and yellow tape}}}} surrounding \Rnode{ss1}{\underline{it}}. There are \Rnode{st}{ \underline{\textcolor{thirdsent}{\textbf{three people in yellow vests and white hard hats}}}}. There are \textcolor{fourthsent}{\textbf{some people sitting on a bench}} next to \Rnode{ss}{\underline{them}}.
\ncbar[arrows = ->, linecolor =blue, angle = -90, arm = 0.9em]{ss}{st}
\ncbar[arrows = ->, linecolor =blue, angle = 90, arm = 1.4em]{ss1}{st1}
\ncbar[arrows = ->, linecolor =blue, angle = 90, arm = 1.4em]{ss2}{st2}
\end{block}
\end{frame}


\begin{frame}{Our paper}
\small
\vspace{-1cm}{How to improve both \textit{accuracy} and \textit{diversity} of generated image paragraphs?}
\begin{minipage}{0.2\textwidth}
\vspace{.5cm}
\includegraphics[width=2.5\textwidth]{hyp}
\end{minipage} \hfill
\begin{minipage}{0.5\textwidth}
\vspace{.5cm}
\begin{itemize}
\item \textbf{model input}:\\ \texttt{unimodal} (visual / textual)\\ vs. \texttt{multimodal}
\pause
\item \textbf{information fusion}:\\ \texttt{max-pooling} vs. \texttt{attention}
\end{itemize}
\end{minipage}
\end{frame}


\begin{frame}{Our paper}
\small
{How to improve both \textit{accuracy} and \textit{diversity} of generated image paragraphs?}
\begin{minipage}{0.2\textwidth}
\vspace{.5cm}
\includegraphics[width=2.5\textwidth]{hyp}
\includegraphics[width=2.5\textwidth]{human}
\end{minipage} \hfill
\begin{minipage}{0.5\textwidth}
\vspace{.5cm}
\begin{itemize}
\item \textbf{model input}:\\ \texttt{unimodal} (visual / language)\\ vs. \texttt{multimodal}
\item \textbf{information fusion}:\\ \texttt{max-pooling} vs. \texttt{attention}
\item \textbf{paragraph evaluation}:\\ \texttt{automatic} vs. \texttt{human}
\pause
\item \textbf{human evaluation}:\\ \texttt{accuracy} and \texttt{diversity} of generated paragraphs
\end{itemize}
\end{minipage}
\end{frame}


\begin{frame}{Unimodal Features: Vision, Language}
\small
We use pre-trained \textbf{DenseCap}\footnote{Johnson, J., Karpathy, A., \& Fei-Fei, L. (2016). DenseCap: Fully Convolutional Localization Networks for Dense Captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.} model to extract both visual ($V$) and language ($L$) features for each image:
\begin{minipage}{1\textwidth}
\vspace{.3cm}
\begin{enumerate}
\item $V \in \setR\sp{M \times D}$: the output of the recognition network (two fully connected layers, within the \textcolor{red}{red box})
\end{enumerate}
\end{minipage} %\hfill
\begin{minipage}{0.6\textwidth}
\vspace{.3cm}
\fbox{\includegraphics[width=1.65\textwidth]{densecap1.pdf}}
\end{minipage}
\vspace{.05cm}\\
Notations: $M=50, D=4096, H=512$.
\end{frame}

\begin{frame}{Unimodal Features: Vision, Language}
\small
We use pre-trained \textbf{DenseCap}\footnote{Johnson, J., Karpathy, A., \& Fei-Fei, L. (2016). DenseCap: Fully Convolutional Localization Networks for Dense Captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.} model to extract both visual ($V$) and language ($L$) features for each image:
\begin{minipage}{1\textwidth}
\vspace{.3cm}
\begin{enumerate}
\item $V \in \setR\sp{M \times D}$: the output of the recognition network (two fully connected layers, within the \textcolor{red}{red box})
\item $L \in \setR\sp{M \times H}$: the sequence of \textit{hidden states} used to generate the region descriptions (within the \textcolor{blue}{blue box})
\end{enumerate}
\end{minipage} %\hfill
\begin{minipage}{0.6\textwidth}
\vspace{.3cm}
\fbox{\includegraphics[width=1.65\textwidth]{densecap2.pdf}}
\end{minipage}
\vspace{.05cm}\\
Notations: $M=50, D=4096, H=512$.
\end{frame}

\iffalse
\begin{frame}{Multimodal Features: Vision \textbf{and} Language}
\small
\includegraphics[width=1\textwidth]{mapvis.pdf}
\end{frame}

\begin{frame}{Multimodal Features: Vision \textbf{and} Language}
\small
\includegraphics[width=1\textwidth]{maplan.pdf}
\end{frame}
\fi

\begin{frame}{Multimodal Features: Vision \textbf{and} Language}
\small
\includegraphics[width=1\textwidth]{maphid.pdf}
\end{frame}

\begin{frame}{Multimodal Features: Vision \textbf{and} Language}
\includegraphics[width=1\textwidth]{maphid.pdf}
\\ 
{\vspace{1cm}\textbf{Note}: passing multimodal features through a linear layer $FC(mult_t)$ did not affect the automatic metric scores.}
\end{frame}

\begin{frame}{Information Fusion: Max-Pooling}
For uni-modal experiments, we use max-pooling on either mapped visual features $x=W_{m}\sp{V} V_t$ or mapped language features $x=W_{m}\sp{L} L_t$:
\begin{equation}
x^\varsigma_s = max_{i=1}^M(x)
\end{equation}
\pause
For multimodal experiments, we concatenate max-pooled vectors of both modalities:
\begin{equation}
x^\varsigma_s = [max_{i=1}^M(W_{m}\sp{L} L_t) \oplus max_{i=1}^M(W_{m}\sp{V} V_t)]
\end{equation}
\end{frame}

\begin{frame}{Information Fusion: Late Attention}

We apply \textbf{additive\textbackslash concat} attention on either unimodal or multimodal features ($F_t$):

\begin{equation}
\alpha_t\sp{mult} = \text{\small $softmax$}(W_a\sp{A} \text{\small $tanh$} (F_t\textcolor{gray}{\oplus W_h h\sp{\delta}_{t-1}})
\end{equation}
\begin{equation}
	f_t = [\alpha_t\sp{mult} \odot F_t ]
\end{equation}
\end{frame}

\begin{frame}{Information Fusion: Late Attention}
\small

We apply \textbf{additive\textbackslash concat} attention on either unimodal or multimodal features ($F_t$):

\begin{equation}
\textcolor{red}{ \alpha_t\sp{mult}} = \text{\small $softmax$}( \textcolor{red}{W_a\sp{A}} \text{\small $tanh$} (F_t\textcolor{gray}{\oplus W_h h\sp{\delta}_{t-1}})
\end{equation}
\begin{equation}
	f_t = [ \textcolor{red}{\alpha_t\sp{mult}} \odot F_t ]
\end{equation}

{\vspace{.5cm}\textbf{Note}: Although some work on multimodal machine translation has shown that early attention improves quality of text generations
\footnote{Ozan Caglayan, Pranava Madhyastha, Lucia Specia, \& Lo\text{\"i}c Barrault. (2019). Probing the Need for Visual Context in Multimodal Machine Translation}$^{,}$\footnote{Ozan Caglayan, Lo\text{\"i}c Barrault, \& Fethi Bougares. (2016). Multimodal Attention for Neural Machine Translation.}
, using \textbf{modality-dependent / early} attention (unique $\textcolor{red}{W_a\sp{A}}$ and, therefore, unique $\textcolor{red}{ \alpha_t\sp{mult}}$ for each modality) provided us with worse automatic metric scores.}

\end{frame}


\begin{frame}{Image Paragraph Model}
\small
\begin{minipage}{.45\textwidth}
\vspace{.3cm}
\includegraphics[width=1\textwidth]{models.pdf}
\end{minipage}\hfill
\begin{minipage}{.5\textwidth}
\begin{itemize}
\item \textbf{IN}: visual / language / multimodal features
\pause
\item \textbf{Discourse LSTM} produces topics for each sentence $n_t \in N$ 
\item \textbf{Sentence LSTM} uses each topic to generate the corresponding sentence
% we do not use any pre-trained embeddings, we learn them from scratch
\pause
\item The model is trained on pairs of images and paragraphs from the Stanford Image Paragraph Dataset
\end{itemize}
\end{minipage} %\hfill
\end{frame}

\begin{frame}{Results: automatic metrics, accuracy}
\small
\includegraphics[width=1\textwidth]{res1.png}
\begin{enumerate}
\item using multimodal features seems to improve the quality of generated paragraphs
\pause
\item max-pooling performs overall better for multimodal features
\end{enumerate}
\end{frame}

\begin{frame}{Results: automatic metrics, diversity}
\small
\center
\includegraphics[width=.7\textwidth]{res2.png}
\begin{enumerate}
\item multimodal features along with attention improve the overall diversity of generated paragraphs
\pause
\item the best performing model is still quite far from the scores for ground-truth paragraphs
\end{enumerate}
\end{frame}

\begin{frame}{Results: human evaluation}
\small
\center
\includegraphics[width=.7\textwidth]{res3.png}
\begin{enumerate}
\pause
\item \textbf{IMG+LNG+MAX} might be a beneficial choice in terms of word choice (WC) and object salience (OS): categories which are directly connected to the accuracy and diversity of paragraphs
\pause
\item models with attention have higher mean scores across all criteria compared to the ones of models with max-pooling
\pause
\item \textbf{LNG+ATT} performs much better than \textbf{IMG+ATT} for sentence structure (SS) and paragraph coherence (PC): categories where semantic information would matter the most
\pause
\item attention seems to affect semantic information more than visual features
\end{enumerate}
\end{frame}

\begin{frame}{Conclusion and Future Work}
\small
\begin{itemize}
\item Multimodal features \textbf{improve} the quality of paragraphs generated by image paragraph models in various ways as judged by both automatic and human evaluation
\pause
\item We need more control over human evaluation, more plausible automatic metrics for diversity
\pause
\item We plan to investigate more the effects of \textbf{early} vs. \textbf{late} information fusion
\pause
\item How would using different decoding strategies (sampling, Nucleus sampling, etc.) affect the quality of paragraphs?
\pause
\item Our goal is to investigate the generation of task-dependent paragraphs (more structured and ordered)
\end{itemize}
\end{frame}

\begin{frame}{}
\center
\textbf{We thank you for your attention!}\\
All code is available at the [github link]
\end{frame}

\end{document}

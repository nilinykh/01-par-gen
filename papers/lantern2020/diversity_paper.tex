%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\newcount\Comments  % 0 suppresses notes to selves in text
\Comments=1   % TODO: set to 0 for final version

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% for comments
\usepackage{color}
\usepackage{multirow}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{purple}{rgb}{1,0,1}
% \kibitz{color}{comment} inserts a colored comment in the text
\newcommand{\kibitz}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
% add yourself here:
\newcommand{\nikolai}[1]{\kibitz{red}      {[Nikolai: #1]}}
\newcommand{\simon}[1]  {\kibitz{blue}   {[Simon: #1]}}
\newcommand{\asad}[1]{\kibitz{purple}     {[Asad: #1]}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hhline}

\newcommand{\R}{\mathbb{R}}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Contextual knowledge is important: utilizing top-down information in generating structured and ordered image paragraphs}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
abstract

\end{abstract}

\section{Introduction}

% Learning discourse from bottom-up and top-down features (?)


Humans are typically able to effortlessly describe real-world images when required: we easily identify objects, attributes and relations between them.
%While describing a scene, we also quickly organise our utterances in the most fitting way possible, editing them while producing when necessary.
Diversity, richness and complexity of such human-produced image descriptions have been observed in several benchmark image description datasets, including MSCOCO \cite{lin2014microsoft,chen2015microsoft}, Flickr8k \cite{hodosh2013}, Flickr30k \cite{young-etal-2014-image}, Visual Genome \cite{krishnavisualgenome}.
These datasets were collected to address the task of automatic image description \cite{bernardi2016automatic}, a long-standing and active field of research, placed in intersection between computer vision and natural language processing (generation, in particular).
This problem of mapping visual data to text can be viewed as the specific example of one of the core goals of NLG: `translating' source data into a natural language representation.

In natural language generation community, the task of text generation has been typically divided into multiple sub-tasks, including \textit{content determination (selection)}, the task of deciding which parts of the source information should be included in the output description, and \textit{text structuring}, the task of ordering selected information \cite{Gatt2017}.
However, with the rise of neural networks in many NLP areas, the generation tasks are now seen as a continuous, non-modular process of automatically learning relations between input and expected output. Specifically, neural models of image captioning \cite{kiros14,vinyals2014tell} are trying to implicitly learn what is important about an image (content selection) and how this information should be structured in the generated caption (text structuring).
Such mechanisms as attention \cite{xu2015attend,anderson2017bottomup} further improve ability of the models to locate important parts in an image and utilize them for caption generation. Some recent advances in image captioning include application of transformer architecture \cite{vaswani2017attention,herdade2019image}.

% SD 2020-07-09 16:50:13 +0200: This is a very good introduction but probably rhe readers of the paper would already know this. We can shorten it later.

While it is clear that neural networks demonstrate good performance in generating \textit{well-structured} single-sentence image captions with \textit{relevant knowledge}, the problem of selecting and ordering information becomes significantly harder when generating multiple sentences for a single image. The corresponding task of
\textit{image paragraph generation} has been initially introduced in \newcite{krause2016hierarchical}, proposing the challenge of generating a text, consisting of several sentences that would form a coherent whole.
Most of the following work \cite{liang2017recurrent,chatterjee2018diverse,wang2019convolutional} has focused on \textit{generating} good, diverse and human-like paragraphs as measured by various automatic evaluation metrics like BLEU \cite{bleu} or CIDEr \cite{vedantam2014cider}.

In this paper we look at the different aspect of image paragraph generation and address the problem of \textbf{information order} in the multi-sentence image captioning setting. % SD 2020-07-09 16:52:39 +0200: Here we could give an example of a picture and text and describe  differences between sentences and therefore explain what we mean by the discourse.
We argue that utilizing top-down knowledge (information about context available to the captioner) is beneficial for the task of image paragraph generation.
We show that the model conditioned on both low-level visual features and high-level top-down information is able to learn human-like distributions of attended objects, attributes and relations generated in the paragraph.
We introduce several image paragraph models based on the hierarchical paragraph generator \newcite{krause2016hierarchical} and also
demonstrate that using bottom-up information exclusively is not enough to learn good paragraph structure.
We employ transfer learning and use model pre-trained for the dense image captioning task \cite{densecap} to obtain representations of background information, which we treat as our top-down features.
We evaluate how close our models are compared to the human performance in terms of attending to objects in visual scenes in a particular order.

% SD 2020-07-09 16:54:27 +0200: Quite a few points here. We can streamline them at the end when we shape the final argument for the paper.

% multimodality
% \cite{barzilay-lee-2004-catching} 

\section{Related Work}

\nikolai{This whole section needs to be shrinked. Or not?}

\paragraph{Human visual attention}
Humans are quite efficient in detecting objects and separating them from the rest of the visual scene \cite{Ullman87}. We are also fluent in using visual attention, which allows us to single out particular objects in the visual scene, significantly reducing our perceptual load when needed \cite{Lavie04}, therefore, preventing us from being overwhelmed by typically complex real-world visual scenes.
Our ability to attend to particular parts of the environment is based on both bottom-up information (low-level visual stimuli) and top-down information (high-level goal-related stimuli, discourse knowledge) \cite{Zarcone2016}. % SD 2020-07-09 16:56:47 +0200: Isn't this paper more about suprisal. There are a lot of refereces about different models of top-down and bottom up attention in psychology that go a long time back: you can find these references in the paper with John.
Stimuli that attracts our attention is said to be \textit{salient} (relevant). Salience of objects affects our \textit{surprisal} towards particular visual input: discourse-salient entities cause less surprisal (e.g. `bed' in bedroom) unlike the visually salient objects (e.g. `large pink elephant' in bedroom).

Attention has also been employed in formal theories of interaction. One of the approaches has been proposed by \newcite{Dobnik2016AMF}, who link attention with judgements as defined in Type Theory of Records \cite{Cooper08typetheory}.
They introduce a Bayesian-based framework in which attention controls the extent to which context induced judgements ($\sim$ task-based top-down information) are utilized by an agent. This allows for topic modelling at each timestamp in interaction.  Thus, it follows along the lines of our proposal about attention using both contextual and low-level visual information in selecting relevant information for each individual sentence in the image paragraph.

% SD 2020-07-09 16:59:36 +0200: As this paper is turning out more like the effects of background knowledge on the discourse structure the discussion of attention is out of the context here. Move the whole section to the attention paper and replace it with a discussion of a discourse structure?


\paragraph{Neural image paragraph captioning}
The task of generating more complex descriptions of images such as paragraphs has been introduced in \newcite{krause2016hierarchical} along with the dataset of image-paragraph pairs.
The paper adopts a hierarchical structure for the model of paragraph generation: sentence RNN is conditioned on visual features and unrolled for each sentence in the paragraph, giving a sentence topic as its output.
Then, each of these topics is used by another RNN to generate actual sentences.
We start by implementing this hierarchical image paragraph model, since it inherits the modular nature of human image paragraph production (given an image, plan structure of your paragraph and identify its sentence topics, then incrementally generate sentences). % SD 2020-07-09 17:04:30 +0200: It also conforms to the theories of NLG: content select and micro-planning.
\newcite{liang2017recurrent} use similar hierarchical network in addition with adversarial discriminator, that forces model to generate realistic paragraphs with smooth transitions between sentences.
\newcite{chatterjee2018diverse} also address cross-sentence topic consistency by modelling global coherence vector, conditioned on all sentence topics.
Different from these approaches, \newcite{melas2019} employ self-critical training technique \cite{selfcritical2016} to directly optimize a target evaluation metric for image paragraph generation.
Lastly, \newcite{wang2019convolutional} use convolutional auto-encoder for topic modelling based on region-level image features. They demonstrate that extracted topics are more representative and contain information relevant for sentence generation.
In this paper we similarly model better topic representations. However, we use additional language representations as part of the input to our topic generator, which is an LSTM.

\paragraph{Language attention in language and vision models}
\nikolai{wordy section, needs to be shorter, keep all information here for now}

Only a limited number of models for image captioning has been supplied with both visual and background information for caption generation.
\newcite{You2016} detect visual concepts found in the scene (objects, attributes, etc.) and extract top-down visual features.
Both of these modalities are then fed to the RNN-based caption generator.
Attention is applied on detected concepts to inform generator about how relevant a particular concept is at each timestamp. Different to their model, we do not use any attribute detectors to identify objects in the scene, instead relying on the output of the model pre-trained for the task of dense captioning.
\newcite{Lu2016} emphasize that image is not always useful in generating some function words (`of', `the', etc.).
They introduce adaptive attention, which determines when to look at the image and when it is more important to use the language model to generate the next word.
In their work, the attention vector is the mixture of visual features and visual sentinel, a vector obtained through the additional gate function on decoder memory state.
Our model is focused on a similar task: we are interested in deciding which type information is more important at a particular timestamp, but we also look at how \textit{merging} two modalities into a single representation performs and how it affects attention of the model.
Closest to our work is the work by \cite{liang2017recurrent}, who apply language attention on region captions and use it to assist recurrent word generation in producing sentences in a paragraph. They embed region descriptions into the same embedding space that their word RNN is operating on. % SD 2020-07-09 17:08:54 +0200: Very similar idea to ours. But we use region descriptions to define topics to choose from (topic selection) rather than directly guide the generation of words (micro-planning).
While we also believe that feeding information about semantic concepts found in an image is beneficial for the model, we propose to employ transfer learning. We use hidden states of the RNN trained for the task of dense captioning \cite{densecap} as our background information representation.
Outside of image paragraph captioning, \newcite{vqaLU16} have proposed a joint image and question attention model for the task of visual question answering.
\nikolai{Any work on language attention in visual dialog? I think one sentence with some citations would be nice to have.}


\section{Approach}
\paragraph{Overview}
As our base model in the experiments, we implement the hierarchical image paragraph generation model by \cite{krause2016hierarchical}.
We change most parts of this model when implementing various model configurations, which we describe below. % SD 2020-07-08 13:38:46 +0200: "Below" made me think this will be in the following section but you mean "here".
To identify image regions and extract their corresponding features,
We also utilize the pre-trained model for dense captioning \cite{densecap}.
For our language representations, we use hidden states from the last layer of the DenseCap RNN, which is supposed to generate region descriptions in the original architecture.
We fuse both modalities into a single vector represented by an affinity matrix, similar to \newcite{Xu2015} and \newcite{vqaLU16}. 
This matrix captures similarities between visual and language representations for all combinations of regions. % SD 2020-07-08 13:36:07 +0200: We need to say a bit more how the fusion is done.
Then, the similarity vector is used by a two-level hierarchical paragraph generator.
First, the sentence-level LSTM transforms its input into the sentence topics, capturing information flow between sentences.
Second, each of the topics is employed by word-level LSTM to generate a sentence.
Finally, all generated sentences per image are concatenated to form a final paragraph. An overview of our model is shown in Fig.
We also deploy various strategies for decoding and analyse differences in corresponding generated paragraphs.
\nikolai{model structure scheme is in progress, not sure there will be space for decoding strategies though}
% SD 2020-07-08 13:42:12 +0200: This would be important as it would be easier fo the reader to see what the differences between us and Krause are. Also, later on when we describe the individual components we can refer back to it.

Note that we were not able to acquire a source code for the original hierarchical model.
Therefore, our model's performance in terms of automatic evaluation might not necessarily be comparable to the one described in \newcite{krause2016hierarchical}.
Though we report automatic evaluation scores, in this paper, we instead focus on showing that background knowledge incorporated in the paragraph generation model leads to more detailed and diverse image descriptions.
% SD 2020-07-08 13:39:42 +0200: This should come earlier at the beginning of the section. As a base we take the model from Krause. We do not have the code so we make our own implementation. Secondly, we also make some modifications because we think they are more suitable. These are: (i)... (ii).... (n) and then their description. We can comment on the differences in results later when we discuss results.

\subsection{Input Features}

\paragraph{Visual Features} Here, we describe our image representation in terms of its regions.
We first dissect our image into multiple salient regions and extract their corresponding convolutional features, using the region detector introduced for the task of dense captioning \cite{densecap}\footnote{Available at: https://github.com/jcjohnson/densecap}.
% SD 2020-07-08 13:44:24 +0200: Here we provide only a brief overview of this 
procedure: first,
First, a resized image is passed through the VGG-16 network \cite{Simonyan2014} to output a feature map of the image.
A region proposal network is conditioned on the feature map to identify the set of salient image regions, which are then mapped back onto the feature map to provide us with corresponding map regions.
Each of these map regions is then fed to the two-layer perceptron to obtain a set of the final region features ${\{v_1, ..., v_M\}}$, with each feature dimension being ${1 \times D}$.
Finally, each image is represented by the matrix of the region features $V \in \R\sp{M \times D}$, which are passed through the simple feed-forward layer $W_v \in \R\sp{D \times H}$ followed by ReLU non-linearity.
These visual feature vectors provide us with fine-grained image representation on the object level.
% SD 2020-07-08 13:45:35 +0200: We don't take the final predictions but an output of some final layer - which ones?

\paragraph{Language Features} As a part of the dense captioning task, a single layer LSTM \cite{lstm97} is conditioned on region features to produce descriptions of these regions in natural language.
We propose to utilize its outputs as language features, which provide us with additional information about detected objects.
Specifically, we condition pretrained LSTM on region features to obtain a set of outputs ${Y = \{y_m, ..., y_M\}}$, where $y_m \in \R\sp{1 \times T \times H}$.
We use mean pooling over the \textit{T} dimension, which determines a number of words in each description and receives a single representation per region. % SD 2020-07-08 13:50:49 +0200: Is the T dimension the final softmax/sigmoid that we then map to words?
Our final matrix of language features per image $L \in \R\sp{M \times H}$ captures semantic information about objects from detected regions. % SD 2020-07-08 13:52:07 +0200: Could come at the beginning to state the motivation why we are using this.
\nikolai{do not forget to mention RNN-GAN work on paragraphs, which also uses densecap regions, but they learn embeddings of words in phrases directly}

\paragraph{Multimodal Features} We wish to leverage language features in our paragraph generation model that is conditioned on visual information.
Similar to \cite{vqaLU16}, we teach our model to co-attend to both of these modalities simultaneously.
Specifically, we use our region feature map $V$ and language features $L$ to compute the following matrix $C$:

\begin{equation}
    C = ReLU(L^TW_vV),
\end{equation}

where $W_v$ is used to learn a mapping from vision space to language space. The final multimodal vector $C \in \R\sp{M \times H}$ is used as a multimodal input to our paragraph generator.
% SD 2020-07-08 13:54:16 +0200: Explain the motivation behind this: we want to create visual and language topics. The dense layer is intended to learn these topics that are supported by visual and language information to a different degree and are relevant for the final training objective.


\subsection{Sentence LSTM}
% SD 2020-07-08 13:56:54 +0200: to be compatible with the story we are presenting let us call this Dsicourse LSTM that predicts the topics on the basis of which sentences are generated and "Word LSTM" Sentence LSTM that relaises individual sentences by predicting the relevant sequences of words.
Our sentence-level LSTM is responsible for modeling topics of each of the individual sentences in the paragraph.
At each timestamp, it is conditioned either on visual or multimodal features, and its output is a set of hidden states $\{h_1, ..., h_S\}$ of length $S$, where each state is used as an input to the word-level LSTM. % SD 2020-07-09 14:02:47 +0200: What is the number of states?
In its nature, sentence LSTM has to simultaneously complete at least two tasks: produce a topic with relevant information for each sentence, while preserving some type of \textit{ordering} between topics.
Such topic ordering is essential for keeping a smooth transition between sentences (discourse items) in the paragraph (mini-discourse).

To assist sentence LSTM in its multiple objectives, we propose to use attention on the sentence topic level.
Attention alleviates the task of weighing specific parts of the input as more important for a particular sentence, thus allowing sentence LSTM to learn more precise sentence representations and sentence order.
In particular, we use soft version of global attention as introduced in \cite{bahdanau2014neural} and applied in image captioning \cite{xu2015attend,luong2015effective}.

\nikolai{I separate between sentence inputs and word inputs by using sigma for sentence input, and omega for word input, original input is simply x}
Let $\oplus$, $\sigma$ and $\odot$ denote concatenation, logistic sigmoid function and element-wise multiplication respectively.
At each time step our attention module receives a feature vector $x$ and previous hidden state $h^\varsigma_{s-1}$ of the sentence LSTM to produce attended input features $\hat{x}^\varsigma_s$.
With both $W_m$ and $W_a$ denoting trainable parameters, attention first computes the attention weights $\alpha_{i,s}$ for each element $x_i$ in the input feature $x$ using additive (concatenative) alignment function as follows:

\begin{equation}
    a(x, h^\varsigma_{s-1})_{i,s} = W_a^\top \tanh (W_m[x_i \oplus h^\varsigma_{s-1}])
\end{equation}

\begin{equation}
    \alpha_s = \frac{exp(a_{i,s})}{\sum_{j}^{M}exp(a_j))}
\end{equation}

Then, the sum of the elements in the combined vector of attention weights and input features is calculated, and passed to the sentence LSTM as its input:

\begin{equation}
    \hat{x}^\varsigma_s = \sum_{i=1}^{M} \alpha_{i,s} \odot x_i
\end{equation}

To demonstrate the difference between simple methods to represent collection of image regions and attention, we also use max-pooling in our experiments to obtain inputs for the sentence LSTM:

\begin{equation}
    \hat{x}^\varsigma_s =  max_{i=1}^M(x)
\end{equation}

% SD 2020-07-09 14:09:53 +0200: State at the beginning that we are going to implement two variants for selection of grounded features: attention and max pooling. Which of these systems is used to report the results?

\subsection{Word LSTM}
% SD 2020-07-09 14:11:46 +0200: See my comment above for the previous section. --> Sentence LSTM
Our word LSTM is a single-layer LSTM similar to \newcite{krause2016hierarchical}.
We create \textit{S} copies of word LSTM (one for each sentence), and use concatenation of the corresponding hidden state of sentence LSTM with the learned embeddings of the words in the target sentence $y_s$ as its input: % SD 2020-07-09 14:15:45 +0200: Copies: the same word-LSTM is trained for every sentence, the same weights are updated.

\begin{equation}
   x^\omega_s = [h^\varsigma_s \oplus Ey_s ]
\end{equation}

% SD 2020-07-09 14:13:58 +0200: The hidden state of the sentence LSTM is concatendated with **every** word embedding vector of the word LSTM?

Our word embedding matrix $E \in \R\sp{K \times H}$ is learned from scratch, $K$ is the vocabulary size. % SD 2020-07-09 14:41:53 +0200: in Krause, the word-ebeddings from DenseCap are taken?
Each copy of word LSTM is unrolled for $M$ timestamps and at each step its hidden step is used to predict a probability distribution over the words in the vocabulary.
The final set of sentences is concatenated together to form a paragraph.

% SD 2020-07-09 14:42:42 +0200: Different implementations of beam search.

\subsection{Learning Objective}
Our hierarchical networks are trained end-to-end with each corresponding image-paragraph pair $(x, y)$ from training data.
Our training loss is a simple cross-entropy loss on the word level:

\begin{equation}
    loss_{\omega}(x, y) = - \sum_{i=1}^{S}\sum_{j=1}^{M_i} log(p_{j,s})
\end{equation}

where $p_{j,s}$ is the softmax probability of the $j^{th}$ word in the $i^{th}$ sentence given all previously generated words for the current sentence $y_{1:j-1,i}$.
For our first sentence, hidden states for both LSTMs are initialized with zeros.
For every next sentence, both LSTMs use last hidden states generated for the previous sentence from the corresponding layers. % SD 2020-07-09 14:44:56 +0200: The hidden state from the word rather than sentence LSTM?
During training, we use teacher forcing and feed ground-truth words as target words at each timestamp.
We use Adam \cite{adam14} as our optimizer and choose the best model based on the validation loss (early stopping).

Note that different from previous work, we do not implement sentence-level loss for the end of paragraph prediction. % SD 2020-07-09 14:46:42 +0200: We do not implent end of paragraph prediction. We should mention this at the beginning of the section where we compare our model with Krause.
Instead, we generate the same number of sentences for each image, as we find in its ground-truth paragraph.
We leave the task of predicting the number of sentences to generate in a paragraph for future work.

\section{Experiments}

We use beam search with beam width $B = 10$ to generate each sentence.
Large beam size is known to favour shorter sentences to the longer ones \cite{Yang2018}.
To avoid this problem, similar to \newcite{opennmt2017}, we ensure that each generated sentence consists of at least $C$ words.
We empirically find that setting $C=10$ provides us with better automatic metric scores.
Also, this number is close to the average sentence length in ground-truth paragraphs (11.91).

\begin{table*}
\footnotesize
\begin{tabular}{|*{15}{l|} }
    \hline
\textbf{Model} 
            & \multicolumn{2}{c|}{\textbf{CIDEr}}
                    & \multicolumn{2}{c|}{\textbf{METEOR}}
                            & \multicolumn{2}{c|}{\textbf{BLEU-1}} 
                   		 & \multicolumn{2}{c|}{\textbf{BLEU-2}}
                      		      & \multicolumn{2}{c|}{\textbf{BLEU-3}} 
                                                & \multicolumn{2}{c|}{\textbf{BLEU-4}}        \\
    \hline
  &   $C\sp{+}$  & $C\sp{-}$  &  $C\sp{+}$  & $C\sp{-}$ & $C\sp{+}$ & $C\sp{-}$  & $C\sp{+}$ & $C\sp{-}$  & $C\sp{+}$ & $C\sp{-}$ & $C\sp{+}$ & $C\sp{-}$  \\
    \hline
\newcite{krause2016hierarchical}   &  -  &   13.52  &   -  &   15.95  &   -  &   41.90  &   -  &   24.11  & - & 14.23 & - & 8.69  \\
    \hline
IMG  &  20.36  &  13.83  &  13.75  &  11.28  &  37.87  &  25.08  &  21.10  &  13.92  & 12.23 & 8.14 & 7.09 & 4.63  \\
    \hline
IMG+ATT   &  \textbf{21.07}  &   14.52  &   13.74  &   11.63  &   37.53  &   26.72  &   20.82  &  15.01  & 12.13 & 8.87 & 7.12 &  5.18  \\
    \hline
LNG   &  19.86  &  12.13  &   13.65 &   10.79  &   37.39  &   23.55 &   20.74  &  12.80  & 11.89 & 7.41 & 6.92 &  4.24  \\
    \hline
LNG+ATT   &  20.30  &  11.37 &   13.76  &   11.00  &   37.68 &   24.48  &   20.76 &  13.32  & 11.85 & 7.65 & 6.82 &  4.31  \\
    \hline    
IMG+LNG   &  20.75 &   13.08  &  \textbf{13.85}  &   11.15 &   37.97  &   24.54  &   21.09  &  13.70  & 12.22 & 8.05 & \textbf{7.18} &  4.69  \\
    \hline
IMG+LNG+ATT   &  20.99 &   13.83  &  13.81  &   11.21 &   \textbf{38.18}  &  25.01  &   \textbf{21.31}  &  13.86  & \textbf{12.25} & 8.08 & 7.12 &  4.67 \\
    \hline
    \end{tabular}
        \caption{
        Scores for automatic evaluations metrics computed for the test set.
        $C\sp{+}$ indicates control for the minimum number of words in generated sentences, $C\sp{-}$ similarly indicates the opposite.
        Scores from the original hierarchical model are reported for completeness (beam search with $C\sp{-}$).
        }
    \label{tab:metrics}
\end{table*}

\iffalse
\begin{table*}[h!]
   % \footnotesize
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
       \multirow{2}{*}\textbf{Model} & \textbf{CIDEr} & \textbf{METEOR} & \textbf{BLEU-1} & \textbf{BLEU-2} & \textbf{BLEU-3} & \textbf{BLEU-4} \\
        & here & \textbf{METEOR} & \textbf{BLEU-1} & \textbf{BLEU-2} & \textbf{BLEU-3} & \textbf{BLEU-4} \\
    \hline
    \hline
    \newcite{krause2016hierarchical} &13.52 & 15.95 & 41.90 & 24.11 & 14.23 & 8.69 \\ 
    \hline
    \hline
      IMG & 20.36 & 13.75 & 37.87 & 21.10 & 12.23 & 7.09 \\
    \hline
      IMG+ATT & \textbf{21.07} & 13.74 & 37.53 & 20.82 & 12.13 & 7.12 \\
    \hline
      LNG & 19.86 & 13.65 & 37.39 & 20.74 & 11.89 & 6.92 \\
    \hline
      LNG+ATT & 20.30 & 13.76 & 37.68 & 20.76 & 11.85 & 6.82 \\
    \hline
     IMG+LNG & 20.75 & \textbf{13.85} & 37.97 & 21.09 & 12.22 & \textbf{7.18} \\
    \hline
     IMG+LNG+ATT & 20.99 & 13.81 & \textbf{38.18} & \textbf{21.31} & \textbf{12.25} & 7.12 \\
    \hline
    \end{tabular}
    \caption{Scores for automatic evaluations metrics computed for the test set.}
    \label{tab:metrics}
\end{table*}
\fi

\section{Evaluation}
Our primary research question is whether supplying image paragraph models with additional background (linguistic) information improves \textit{diversity} of generated paragraphs.
At the same time, note that paragraphs must have an acceptable level of \textit{accuracy} (correctness in describing an image).
However, image description systems might produce an accurate description, which does not necessarily correspond to the ground truth paragraph from the dataset, since there are multiple correct ways of describing an image \nikolai{some reference here}.
This would affect the final automatic evaluation scores, which compare generated descriptions with the ground truth ones, and, therefore, may not be good measures of either accuracy or diversity.
We take this into account and report results of human evaluation (section reference) of generated paragraphs in terms of both accuracy and diversity.
For completeness, we also report results of automatic evaluation (section reference) and demonstrate that automatic metrics fail to capture human level of judgements.

\subsection{Definition of Diversity}
Image paragraph generation is an open-ended generation task, in which, in contrast to single sentence captions, generated paragraphs must be highly informative, thus, include descriptions of a large variety of image objects, attributes, etc.
Here, we refer to this paragraph feature as \textbf{diversity}.
In language and vision literature, "diversity" of image descriptions has been mostly defined in terms of lexical diversity, word choice and \textit{n}-gram based metrics \cite{Devlin2015, Vijayakumar2016, Lindh2018, VanMiltenburg2018}.
These criteria are focused on generating diverse set of \textit{independent, one-sentence captions}, with each describing image as a whole.
These captions are very likely to mention identical objects due to the nature of the task ("describe image with a single sentence"), and diversity is measured in terms of how different object descriptions are from one caption to another (e.g. 'man' can be described as a 'person', 'human').
However, a good image paragraph model must also introduce diversity on the sentence level, since describing \textit{different scene objects} throughout the paragraph is what makes it more informative than single sentence captions.
Here, we define \textit{paragraph diversity} with two requirements: a generative model must (a) produce the set of sentences with reasonable mentions of a variety of image objects (sentence-level diversity), (ii) demonstrate the ability to use many different words to describe objects without unnecessary repetitions (word-level diversity).

% SD 2020-07-09 14:49:20 +0200: Very good point to distinguish the two, however.... The system can also produce an accurate description but which does not correspond to the ground truth in the dataset. The automatic measures that we have under accuracy therefore may not be good measures of accuracy. They are measures of faithufllness to the ground-truth description. Perhaps we should reserve the accuracy and diversity distinction for human evaliation and here report only automatic measures and conclude that they are defifciient in showing eccuracy and diversity and therefore human evaluation.

\subsection{Metrics}

\paragraph{Accuracy}
Typically, a variety of n-gram based automatic metrics is used to measure the correctness/accuracy of image captions.
Here, we evaluate our models across six different metrics: CIDEr \cite{vedantam2014cider}, METEOR \cite{meteor14}, and BLEU-\{1, 2, 3, 4\} \cite{bleu}.
\nikolai{add a note about WMD and calculate WMD}

\paragraph{Diversity}
Majority of the diversity metrics for image descriptions are based on calculating \textit{n}-gram statistics as well as comparing vocabulary size, POS usage etc.
Here, we report vocabulary size of our models and also the number of unique nouns which are generated as an approximation of how many unique objects are mentioned in the paragraphs. We use spaCy \cite{spacy15} to identify nouns in generated paragraphs.
We also calculate Self-BLEU \cite{Zhu2018selfbleu} or sometimes referred to as mBleu \cite{Shetty2017}.
This metric has been specifically proposed to assess the similarity between two sentences, and it can be used to measure how much one sentence resembles another. This is in particular important for the paragraphs, in which sentences should not be very similar with each other.
Higher self-Bleu indicates less diversity, e.g., more n-gram matches between sentences.
We calculate Self-BLEU as follows: we split each generated paragraph into sentences and use one sentence as hypothesis and the others are regarded as references.
Then, BLEU score is calculated for every sentence in every paragraph, and the average BLEU score over the paragraphs is used as the Self-BLEU score of the whole set.

\subsection{Results}

\paragraph{Accuracy}
As Table~\ref{tab:metrics} demonstrates, all our models without controlling for $C$ show similar performance and slightly underperform the model by \newcite{krause2016hierarchical} in all metrics except CIDEr.
We believe that there are several reasons for such behaviour.
First, our model slightly differs from the original hierarchical model: we do not use any pre-trained weights and do not learn to predict end of the paragraph.
Also, we utilise attention in LSTM-based text generator, which, as has been shown by \newcite{liang2017recurrent} and \newcite{wang2019convolutional}, significantly boosts performance in terms of CIDEr score.

The obvious trend is that models which are forced to generate at least $C$ words in a sentence perform much better, improving all accuracy-related scores.
However, it is unclear whether this will have a positive affect on diversity measures as well: automatic evaluation metrics might be biased towards favouring longer sentences even if they are repetitive.
\nikolai{ref!}

We also conclude that adding modalities (IMG+LNG) somewhat improves scores by a small margin compared to using one of the two (either IMG or LNG).
Also, attention seems to boost performance of either unimodal or multimodal systems.
Interestingly, LNG-based model performs slightly worse across almost all metrics.
We believe that the quality of linguistic representations affects automatic scores, indicating that more complex linguistic representations are needed.
Currently, language input is constructed from representations used to describe only specific regions in isolation.
In the paragraph, however, these regions must be linked to each other to form a coherent whole.
We leave experiments with such linguistics representations for the future work.


\begin{table*}
\footnotesize
\begin{tabular}{|*{15}{l|} }
    \hline
\textbf{Model} 
            & \multicolumn{2}{c|}{\textbf{Voc Size}}
                    & \multicolumn{2}{c|}{\textbf{\# of NT}}
                            & \multicolumn{2}{c|}{\textbf{SB-1}} 
                   		 & \multicolumn{2}{c|}{\textbf{SB-2}}
                      		      & \multicolumn{2}{c|}{\textbf{SB-3}} 
                                                & \multicolumn{2}{c|}{\textbf{SB-4}}        \\
    \hline
  &   $C\sp{+}$  & $C\sp{-}$  &  $C\sp{+}$  & $C\sp{-}$ & $C\sp{+}$ & $C\sp{-}$  & $C\sp{+}$ & $C\sp{-}$  & $C\sp{+}$ & $C\sp{-}$ & $C\sp{+}$ & $C\sp{-}$  \\
    \hline
IMG  &  378  &  430 &  284  &  313  & 80.33 &  70.62  & 71.75  &  58.99  & 64.84 & 50.85 & 58.71 & 44.11  \\
    \hline
IMG+ATT   & 296 &  435  &  295 &  315 &  79.72  &  71.07  &   71.05  &  59.69  & 64.05 & 51.53 & 57.92 & 44.63 \\
    \hline
LNG   &  399  &  431  &  294 &  310 &  77.58  &  68.69 &   68.26 &  57.24  & 60.74 & 49.41 & 54.45 & 42.79 \\
    \hline
LNG+ATT   &  269 &  430 &   295 &  308  &  77.85 &   69.78  &  68.44  &  58.35  & 61.23 & 50.42 & 55.31 & 43.85  \\
    \hline    
IMG+LNG   &  283 &   420 & 295  &  299 &  79.22  &   71.10  &   70.56  &  59.85 & 63.65 & 51.79 & 57.72 & 45.05  \\
    \hline
IMG+LNG+ATT   &  413 & \textbf{452} &  300  &  \textbf{326} & 77.81 &  \textbf{68.25}  &  68.19  &  \textbf{55.97}  & 61.06 & \textbf{47.61} & 55.23 & \textbf{40.98} \\
\hline
    GT & - & \textbf{5835} & - & \textbf{3865} & - & \textbf{48.66} & - & \textbf{27.95} & - & \textbf{15.82} & - & \textbf{8.70} \\
    \hline
    \end{tabular}
        \caption{Measures of diversity for different paragraph models. NT and GT stand for noun types and ground-truth paragraphs from the test set respectively. SB stands for self-BLEU and corresponding \textit{n}-gram (1, 2, 3, 4).}
    \label{tab:divs}
\end{table*}

\iffalse
\begin{table*}[h]
   % \footnotesize
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
       \textbf{Model} & \textbf{Voc Size} & \textbf{\# of NT} & \textbf{SB-1} & \textbf{SB-2} & \textbf{SB-3} & \textbf{SB-4} \\
    \hline
      IMG & 378 & 284 & 80.33 & 71.75 & 64.84 & 58.71 \\
    \hline
      IMG+ATT & 296 & 295 & 79.72 & 71.05 & 64.05 & 57.92 \\
    \hline
      LNG & 399 & 294 & \textbf{77.58} & 68.26 & \textbf{60.74} & \textbf{54.45} \\
    \hline
      LNG+ATT & 269 & 295 & 77.85 & 68.44 & 61.23 & 55.31 \\
    \hline
     IMG+LNG & 283 & 295 & 79.22 & 70.56 & 63.65 & 57.72 \\
    \hline
     IMG+LNG+ATT & \textbf{413} & \textbf{300} & 77.81 & \textbf{68.19} & 61.06 & 55.23 \\
    \hline
      GT & \textbf{5835} & \textbf{3865} & \textbf{48.66} & \textbf{27.95} & \textbf{15.82} & \textbf{8.70} \\
    \hline
    \end{tabular}
    \caption{Measures of diversity for different paragraph models. NT and GT stand for noun types and ground-truth paragraphs from the test set respectively. SB stands for self-BLEU and corresponding \textit{n}-gram (1, 2, 3, 4).}
    \label{tab:stats}
\end{table*}
\fi

\paragraph{Diversity}
Note that our goal is to show that purely visually dependent models are less diverse than models which incorporate additional sources of information such as semantic information about objects or use attention.
As Table ~\ref{tab:divs} demonstrates, the model which uses both modalities and attention generates most diverse paragraphs compared to outputs of the other models.
It generates more unique nouns, has bigger vocabulary size and better self-BLEU scores.
However, it still performs much worse than the human-generated paragraphs.
Based on the self-BLEU scores for models which are forced to generate at least $C$ words, we conclude that these models tend to generate the same \textit{n}-grams more often.
Therefore, these models suffer from repetitiveness on word level, and do not conform with our criteria for paragraph diversity.

% SD 2020-07-09 16:11:33 +0200: The diversity should be **lagom** not too little diverse and not overly divserse given human-generated descriptions.

\subsection{Diversity in decoding}
diversity in terms of choosing best candidate from the pool of candidates. Experiment with choosing different candidates (not always the most probable) and report how diversity changes

\subsection{Human Evaluation}
Human evaluation on 

% SD 2020-07-09 16:18:09 +0200: Self-BLEU and other measures in Table 2 are good but they evaluate vocabulary only. However, as our paper is about a discourse we also need to evaluate whether the paragraphs have a better discourse structure. How shall we do that? This would be the main purpose of the human-evaluation experiment.


% write up section on self bleu
% temperature-related experiments (?)
% compare all model generations for quality/diversity with temperature, make a graph with curves
% the importance of temperature for evaluating models based on quality and diversity
% using temperature as overall evaluation of models in their quality and diversity


% T = 15
% D = 4096
% M = 50
% S = 6
% K = vocabulary length

%As the next step, \newcite{krause2016hierarchical} use max pooling on region features to obtain a single vector $v_p \in \R\sp{P}$, which would capture the most important information about image as a whole.
%While this approach would make individual image regions highly salient, it is unclear whether this would capture importance of many other important objects described in the multi-sentence image description.
%As an alternative, mean pooling can be used to describe image as a set of regions. However, this would scatter the model's focus on many different regions, which could affect its ability to attend to the most important regions when generating a sentence.

\section{Conclusion}

\bibliography{diversity}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendices}
\label{sec:appendix}
Appendices are material that can be read, and include lemmas, formulas, proofs, and tables that are not critical to the reading and understanding of the paper. 
Appendices should be \textbf{uploaded as supplementary material} when submitting the paper for review.
Upon acceptance, the appendices come after the references, as shown here.

\paragraph{\LaTeX-specific details:}
Use {\small\verb|\appendix|} before any appendix section to switch the section numbering over to letters.


\section{Supplemental Material}
\label{sec:supplemental}
Submissions may include non-readable supplementary material used in the work and described in the paper.
Any accompanying software and/or data should include licenses and documentation of research review as appropriate.
Supplementary material may report preprocessing decisions, model parameters, and other details necessary for the replication of the experiments reported in the paper.
Seemingly small preprocessing decisions can sometimes make a large difference in performance, so it is crucial to record such decisions to precisely characterize state-of-the-art methods. 

Nonetheless, supplementary material should be supplementary (rather than central) to the paper.
\textbf{Submissions that misuse the supplementary material may be rejected without review.}
Supplementary material may include explanations or details of proofs or derivations that do not fit into the paper, lists of
features or feature templates, sample inputs and outputs for a system, pseudo-code or source code, and data.
(Source code and data should be separate uploads, rather than part of the paper).

The paper should not rely on the supplementary material: while the paper may refer to and cite the supplementary material and the supplementary material will be available to the reviewers, they will not be asked to review the supplementary material.

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
